**Projeto de Fun√ß√µes de Ativa√ß√£o em Redes Neurais**

Este projeto visa explorar diferentes fun√ß√µes de ativa√ß√£o comumente usadas em redes neurais artificiais. As fun√ß√µes de ativa√ß√£o desempenham um papel crucial no processo de aprendizado de uma rede neural, pois introduzem n√£o-linearidades que permitem que a rede aprenda rela√ß√µes complexas nos dados.

**Conte√∫do do Projeto:**

1. **Fun√ß√µes de Ativa√ß√£o Implementadas:**
   - ReLU (Unidade Linear Retificada)
   - Leaky ReLU (Leaky Unidade Linear Retificada)
   - Sigmoide
   - Tangente Hiperb√≥lica
   - Gaussiana
   - Linear (Purelin)

2. **Implementa√ß√£o em Python:**
   O projeto inclui a implementa√ß√£o de cada fun√ß√£o de ativa√ß√£o em Python, juntamente com suas derivadas quando aplic√°vel. O c√≥digo est√° organizado em fun√ß√µes modulares para facilitar a compreens√£o e a reutiliza√ß√£o.

**Instru√ß√µes de Uso:**

1. Clone o reposit√≥rio para o seu Google Colab
2. Execute o c√≥digo Python para visualizar os gr√°ficos das fun√ß√µes de ativa√ß√£o e suas derivadas.
3. Experimente com diferentes intervalos de entrada e par√¢metros para obter insights adicionais sobre o comportamento das fun√ß√µes.
4. 
**Autores:**
  - [Caroline Souza](https://github.com/caahp)
  - [Kirk Sahdo](https://github.com/kirksahdo)

üöÄüß†
